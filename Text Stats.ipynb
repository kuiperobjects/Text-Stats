{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95365391",
   "metadata": {},
   "source": [
    "<h1> Text Statistics Functions</h1> \n",
    "These quantitative linguistics measures come from automatic scoring and testing literature and can be used for a wide variety of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f8f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import random\n",
    "import collections\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "import sys\n",
    "#filename=sys.argv[1]\n",
    "#raw = open('/filename', \"r\", encoding=\"utf-8-sig\")\n",
    "\n",
    "raw = open('...testdoc.txt', encoding='utf-8').read()\n",
    "#doc= nlp(raw)\n",
    "\n",
    "#Spanish #Hindi #Japanese #Portuguese #Russian #Finnish #Malayalam #Urdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "These subroutines are features of readability measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize words in document, add to list of words, then print how many there are\n",
    "words=[]\n",
    "for word in doc:\n",
    "    words.append(word)\n",
    "#print(\"This document contains \" + str(len(words)) + \" words.\")\n",
    "\n",
    "# word tokenization in pure python \n",
    "#pp_words = raw.split()\n",
    "#len(pp_words)\n",
    "\n",
    "# sentence tokenization in pure python \n",
    "#pp_sentences = [raw.split('.')]\n",
    "#pp_sentences\n",
    "\n",
    "# tokenize sentences in document, add to list of sentences, then print how many there are\n",
    "sentences = []\n",
    "for sent in doc.sents:\n",
    "    sentences.append(sent)\n",
    "print(\"This document contains \" + str(len(sentences)) + \" sentences.\")\n",
    "\n",
    "\n",
    "# English count syllables \n",
    "# Need to rewrite syllable counter to work for: Spanish, Hindi, Urdu, Malayalam,\n",
    "# Finnish, Japanese, Portuguese, and Russian.\n",
    "def syllables(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = str(word).lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return count\n",
    "\n",
    "# avg sentence sentence length = sum of words in doc / sum of sentences in doc\n",
    "ASL = len(words) / len(sentences)\n",
    "print(\"AVG sent length: \" + str(ASL))\n",
    "\n",
    "# number of syllables in each word\n",
    "syl = [syllables(word) for word in words]\n",
    "\n",
    "# avg number of syllables in a doc =  sum of syllables in each word / sum of words in a doc\n",
    "ASW = sum(syl)/len(words)\n",
    "print(\"AVG syllable length: \" + str(ASW))\n",
    "\n",
    "# number of chars in each word  \n",
    "# need to preprocess to edit punc or minus 1 for words at end of sent\n",
    "word_char_length = [len(word) for word in words]\n",
    "\n",
    "# avg number of chars in word = sum of chars in each word / sum of words in doc\n",
    "ACW = sum(word_char_length) / len(words)\n",
    "\n",
    "# big words = words with 3 or more syllables\n",
    "bigwords = []\n",
    "for word in words:\n",
    "    #print(syllables(x))\n",
    "    if syllables(word.text) >= 3:\n",
    "       bigwords.append(word) \n",
    "\n",
    "# easy words = words with 2 or 1 syllables\n",
    "easywords = []\n",
    "for word in words:\n",
    "    #print(syllables(x))\n",
    "    if syllables(word.text) <= 2:\n",
    "       easywords.append(word) \n",
    "\n",
    "\n",
    "# count how many times each syllable count occurs in the doc\n",
    "import collections\n",
    "counter = collections.Counter(syl)\n",
    "#print(\"this document contains words with syllables that occur this many times: \" \n",
    "    #  + str(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f4380",
   "metadata": {},
   "source": [
    " <h4> Reading meaures measure the difficulty of a text. </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cdfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadingMeasures:\n",
    "    \n",
    "    # Gunning Fog Formula: %Bigwords + ASL * 0.4\n",
    "    def gunning_fog():\n",
    "        gunning_fog = len(bigwords)/len(words) + (ASL*0.4)\n",
    "        return gunning_fog\n",
    "    \n",
    "    # Flesch Readability Formula: 206.835 - (1.05* ASL) - (84.6 * ASW)\n",
    "    def flesch(): \n",
    "         flesch = 206.835 - (1.05* ASL) - (84.6 * ASW)\n",
    "         return flesch\n",
    "    \n",
    "    # Flesch-Kincard Formula: (0.39 * ASL ) + (11.8 * ASW) - 15.59\n",
    "    def flesch_kinkaid():\n",
    "        flesch_kinkaid = (0.39 * ASL ) + (11.8 * ASW) - 15.59\n",
    "        return flesch_kinkaid\n",
    "    \n",
    "    # SMOG Formula: 3 + sqrt(sum(bigwords))\n",
    "    def smog():\n",
    "        sentence_sample = []\n",
    "        biguns = []\n",
    "        try: \n",
    "            if len(sentences) >= 30:\n",
    "            # get first ten sentences in a doc\n",
    "            sentence_sample.extend(sentences[0:10])\n",
    "            len(sentence_sample)\n",
    "\n",
    "            # get middle ten sentences in a doc \n",
    "            sentence_sample.extend(sentences[round((len(sentences)/3)):(round((len(sentences)/3))+10)])\n",
    "            len(sentence_sample)\n",
    "\n",
    "            # get last ten sentences in a doc\n",
    "            sentence_sample.extend(sentences[-10:])\n",
    "            len(sentence_sample)\n",
    "\n",
    "\n",
    "            for sent in sentence_sample:\n",
    "                for word in sent:\n",
    "                    if syllables(word.text) >= 3:\n",
    "                        #print(word.text)\n",
    "                    biguns.append(word.text)\n",
    "                    #biguns = biguns+1\n",
    "            \n",
    "        smog = 3 + math.sqrt(len(biguns))\n",
    "        return smog\n",
    "    except:\n",
    "        print(\"Document length must at least 30 sentences long in order to use this statistic\")\n",
    "    \n",
    "    \n",
    "    # Automated-Readability Formula: 4.71(ACW) + 0.5(ASW) - 21.43\n",
    "    def ari():\n",
    "        ari = 4.71(ACW) + 0.5(ASW) - 21.43\n",
    "        return ari\n",
    "    \n",
    "    # Coleman-Liao Formula: 0.0588*L - 0.296*S - 15.8\n",
    "    def coleman_liao():\n",
    "        coleman_liao = ((0.0588*L) - (0.296*S)) - 15.8\n",
    "        return coleman_liao\n",
    "        \n",
    "    # Linsear-Write 100 word sample (easywords * 1)  + (hardwords * 3) / len(sentences)\n",
    "    def linsear_write(self, sample_size):\n",
    "       \n",
    "        # grab a random sample of 100 word tokens \n",
    "        random_bow = random.sample(words, 100)\n",
    "        random_bow = [word.text for word in random_bow]\n",
    "    \n",
    "        linsear_write.easywords = []\n",
    "        linsear_write.bigwords = []\n",
    "       \n",
    "        for word in wordlist:\n",
    "            if syllables(word) <= 2:\n",
    "               linsear_write.easywords.append(word)\n",
    "            elif syllables(word)>=3:\n",
    "               linsear_write.bigwords.append(word)    \n",
    "    \n",
    "        linsear_write = ((len(linsear_write.easywords * 1 ) + (len(linsear_write.bigwords)*3) / (len(sentences)) / 2))\n",
    "        if linsear_write > 20: \n",
    "            return linsear_write/2\n",
    "        elif linsear_write <=20:\n",
    "            return (linsear_write-2)/2\n",
    "        \n",
    "    # FORCAST Grade Level = 20 (N/10) ## N = number of words in a rand sample that are a single syllable\n",
    "    def forcast():\n",
    "        forcast.single_syll = []\n",
    "        random_bow = random.sample(words, 150)\n",
    "        for word in random_bow:\n",
    "            if syllables(word) == 1:\n",
    "                forcast.single_syll.append(word)\n",
    "                \n",
    "        forcast = 20 (len(forcast.single_syll)/10)\n",
    "        return forcast\n",
    "    \n",
    "    # New Dale Chall Formula = (0.1579 * (PDW)) + 0.0496 *((ASL))\n",
    "    def new_dale_chall():\n",
    "        new_dc = (0.1579 * (PDW)) + 0.0496 *((ASL))\n",
    "        return new_dc\n",
    "           \n",
    "    # Fucks Stilcharateristisk Formula: AWL * ASL  ## AWL = ACW\n",
    "    def fucks():\n",
    "        fucks = ACW * ASL \n",
    "        return fucks\n",
    "        \n",
    "    # Dickes-Steiwer Handformel = 235.95993−(7.3021×AWL)−(12.56438×ASL)−(50.03293×TTR)\n",
    "    def dickes_steiwer():\n",
    "        dickes_steiwer =  235.95993−(7.3021×AWL)−(12.56438×ASL)−(50.03293×TTR)\n",
    "        return dickes_steiwer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
